# H_teste_manual_llm.py - FERRAMENTA DE TESTE MANUAL SERPRO LLM
"""
UTILIT√ÅRIO INTERATIVO PARA TESTE DIRETO DO SERPRO LLM

Esta ferramenta de linha de comando permite:
1. Testar justificativas diretamente no Serpro LLM
2. Verificar conectividade e autentica√ß√£o
3. Analisar respostas do LLM em tempo real
4. Debugging e desenvolvimento de prompts
5. Valida√ß√£o de configura√ß√µes
6. Processamento de linhas completas com salvamento autom√°tico

PROP√ìSITO:
- Ferramenta de desenvolvimento e debugging
- Teste r√°pido de prompts e justificativas
- Verifica√ß√£o de conectividade com Serpro LLM
- An√°lise de qualidade das respostas
- Prototipagem de novos prompts
- Processamento individual com salvamento em JSON

FORMATOS DE ENTRADA ACEITOS:
1. Justificativa simples: "Texto da justificativa aqui"
2. Linha completa: "IDTERMO#CPF#PRATICA VEDADA#JUSTIFICATIVA"

DIFEREN√áAS DA API WEB:
- Modo interativo via linha de comando
- Sem interface web ou WebSocket
- Foco em teste e debugging
- Execu√ß√£o √∫nica por vez
- Feedback detalhado sobre parsing
- Salvamento autom√°tico em JSON quando linha completa

FLUXO DE EXECU√á√ÉO:
1. Configura√ß√£o inicial e autentica√ß√£o
2. Loop interativo para entrada de justificativas/linhas
3. Para cada entrada:
   - Detec√ß√£o do formato (simples ou completa)
   - Parsing da linha se formato completo
   - Cria√ß√£o do prompt especializado
   - Chamada ao Serpro LLM
   - Parsing da resposta (m√∫ltiplas estrat√©gias)
   - Salvamento em JSON se linha completa
   - Exibi√ß√£o formatada dos resultados
4. Op√ß√£o de continuar ou sair
"""

# ========== IMPORTS E DEPEND√äNCIAS ==========
import asyncio         # Para programa√ß√£o ass√≠ncrona (chamadas LLM)
import aiohttp          # Cliente HTTP ass√≠ncrono para Serpro LLM
import requests         # Cliente HTTP s√≠ncrono (autentica√ß√£o)
import json             # Para parsing e formata√ß√£o JSON
import os               # Para sistema de arquivos e vari√°veis ambiente
import sys              # Para manipula√ß√£o de imports
import time             # Para medi√ß√£o de tempo de resposta
from datetime import datetime  # Para timestamps
import uuid             # Para gera√ß√£o de IDs √∫nicos
import re               # Para regex (extra√ß√£o JSON de texto)
from pathlib import Path       # Para manipula√ß√£o de caminhos
from dataclasses import dataclass, asdict  # Para estruturas de dados

# ========== IMPORT DA CONFIGURA√á√ÉO CENTRALIZADA ==========
# Importa√ß√£o din√¢mica do arquivo 0_config.py para reutilizar configura√ß√µes
sys.path.append(os.path.dirname(__file__))
import importlib.util
spec = importlib.util.spec_from_file_location("config", "0_config.py")
config_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(config_module)
SerproConfig = config_module.SerproConfig

# ========== ESTRUTURAS DE DADOS ==========

@dataclass
class TesteResult:
    """
    RESULTADO ESTRUTURADO DO TESTE MANUAL
    
    Armazena todas as informa√ß√µes sobre o processamento de uma justificativa:
    - Dados originais (ID, CPF, pr√°tica vedada, justificativa)  
    - Resultado da an√°lise LLM (diagn√≥stico, confian√ßa, justificativa)
    - Metadados (tempo, modelo, ambiente, fallback)
    
    Similar ao ProcessingResult do E_processador_arquivo.py mas
    adaptado para teste manual com informa√ß√µes adicionais.
    """
    id_termo: str = "MANUAL"               # ID do termo (ou "MANUAL" se justificativa simples)
    cpf: str = ""                          # CPF do usu√°rio
    pratica_vedada: str = ""               # Tipo de pr√°tica vedada
    justificativa: str = ""                # Justificativa completa do usu√°rio
    diagnostico_llm: str = ""              # Resposta do LLM: SIM/N√ÉO
    confidence: float = 0.0                # N√≠vel de confian√ßa (0.0 a 1.0)
    justificativa_llm: str = ""            # Explica√ß√£o do LLM sobre a decis√£o
    processing_time: float = 0.0           # Tempo gasto no processamento (segundos)
    timestamp: str = ""                    # Timestamp ISO 8601 do processamento
    model_used: str = ""                   # Modelo LLM utilizado
    ambiente_serpro: str = ""              # Ambiente Serpro (exp/prod)
    fallback_used: bool = False            # Se foi usado fallback sem√¢ntico
    request_id: str = ""                   # ID √∫nico da requisi√ß√£o
    
    def __post_init__(self):
        """Gera timestamp e request_id automaticamente se n√£o fornecidos"""
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()
        if not self.request_id:
            self.request_id = str(uuid.uuid4())

# ========== CLASSE PRINCIPAL DE TESTE ==========

class TesteLLMManual:
    """
    CLASSE PRINCIPAL PARA TESTE MANUAL DO SERPRO LLM
    
    Implementa um cliente simplificado e direto para:
    - Autentica√ß√£o OAuth2 com Serpro LLM
    - Configura√ß√£o autom√°tica de certificados SSL
    - Cria√ß√£o de prompts especializados
    - Chamadas ass√≠ncronas ao LLM
    - Parsing inteligente de respostas
    - Fallback para respostas n√£o-JSON
    
    CARACTER√çSTICAS:
    - Interface simples focada em teste
    - Sem sistema de retry complexo (para debugging)
    - Feedback detalhado sobre cada etapa
    - Medi√ß√£o de tempo de resposta
    - Detec√ß√£o de uso de fallback
    
    DIFEREN√áAS DA VERS√ÉO PRODU√á√ÉO:
    - Sem sistema robusto de retry
    - Sem monitoramento de estat√≠sticas
    - Sem categoriza√ß√£o avan√ßada de erros
    - Foco em simplicidade e clareza
    """
    
    def __init__(self):
        """
        INICIALIZA√á√ÉO DA CLASSE DE TESTE
        
        1. Carrega configura√ß√µes centralizadas
        2. Inicializa token como None (ser√° obtido quando necess√°rio)
        3. Configura certificados SSL do Serpro
        4. Configura pasta de sa√≠da JSON
        """
        # Carregar configura√ß√µes do arquivo 0_config.py
        self.config = SerproConfig()
        
        # Token de acesso (ser√° obtido dinamicamente)
        self.access_token = None
        
        # Configurar certificados SSL automaticamente
        self.setup_certificates()
        
        # Configurar pasta de sa√≠da JSON
        self.setup_output_folder()
        
    def setup_output_folder(self):
        """
        CONFIGURA√á√ÉO DA PASTA DE SA√çDA JSON
        
        Cria pasta ./JSON se n√£o existir para salvar resultados
        individuais quando processando linhas completas.
        """
        self.json_folder = Path("./JSON")
        self.json_folder.mkdir(exist_ok=True)
        
    def detect_input_format(self, entrada: str) -> str:
        """
        DETEC√á√ÉO INTELIGENTE DO FORMATO DE ENTRADA
        
        Identifica se a entrada √©:
        1. Linha completa: IDTERMO#CPF#PRATICA VEDADA#JUSTIFICATIVA (4+ campos)
        2. Justificativa simples: Texto livre
        
        Args:
            entrada: String de entrada do usu√°rio
            
        Returns:
            str: "linha_completa" ou "justificativa_simples"
        """
        # Contar campos separados por #
        campos = entrada.split("#")
        
        # Se tem 4 ou mais campos, considerar linha completa
        if len(campos) >= 4:
            return "linha_completa"
        else:
            return "justificativa_simples"
    
    def parse_linha_completa(self, linha: str) -> dict:
        """
        PARSING DE LINHA COMPLETA NO FORMATO PADR√ÉO
        
        Converte string no formato: IDTERMO#CPF#PRATICA VEDADA#JUSTIFICATIVA
        Para dicion√°rio com campos nomeados.
        
        TRATAMENTO ESPECIAL:
        - Se justificativa cont√©m #, preserva o conte√∫do usando join()
        - Exemplo: "123#456#12#Texto com # no meio" ‚Üí justificativa = "Texto com # no meio"
        
        Args:
            linha: String no formato delimitado por #
            
        Returns:
            dict: Campos estruturados
            
        Raises:
            ValueError: Se formato da linha for inv√°lido (menos de 4 campos)
        """
        parts = linha.split("#")
        if len(parts) < 4:
            raise ValueError(f"Formato inv√°lido. Esperado: IDTERMO#CPF#PRATICA VEDADA#JUSTIFICATIVA. Recebido: {linha}")
        
        return {
            "id_termo": parts[0].strip(),
            "cpf": parts[1].strip(),
            "pratica_vedada": parts[2].strip(),
            "justificativa": "#".join(parts[3:]).strip()  # Join caso justificativa contenha #
        }
        
    def setup_certificates(self):
        """
        CONFIGURA√á√ÉO AUTOM√ÅTICA DE CERTIFICADOS SSL SERPRO
        
        O Serpro requer certificados espec√≠ficos para conex√µes HTTPS.
        Este m√©todo:
        1. Verifica se certificado j√° existe localmente
        2. Se n√£o existe, baixa automaticamente do site oficial
        3. Configura vari√°veis de ambiente para uso pelo requests/aiohttp
        
        CERTIFICADO SERPRO:
        - URL oficial: https://lcrspo.serpro.gov.br/ca/ca-pro.pem
        - Arquivo local: ca-pro.pem
        - Necess√°rio para todas as conex√µes HTTPS com Serpro
        
        Raises:
            Exception: Se falhar ao baixar ou configurar certificado
        """
        cert_file = self.config.CERT_FILE
        
        # Verificar se certificado j√° existe
        if not os.path.exists(cert_file):
            try:
                print("üì• Baixando certificado SSL...")
                
                # Baixar certificado oficial (temporariamente sem verifica√ß√£o SSL)
                response = requests.get(self.config.CERT_URL, verify=False, timeout=10)
                response.raise_for_status()
                
                # Salvar certificado localmente
                with open(cert_file, 'wb') as f:
                    f.write(response.content)
                print("‚úÖ Certificado SSL configurado")
                
            except Exception as e:
                print(f"‚ùå Erro ao baixar certificado: {e}")
                raise
        
        # Configurar vari√°veis de ambiente para uso do certificado
        os.environ["REQUESTS_CA_BUNDLE"] = cert_file
        os.environ["SSL_CERT_FILE"] = cert_file
    
    def get_access_token(self):
        """
        OBTEN√á√ÉO DE TOKEN DE ACESSO OAUTH2
        
        Implementa fluxo OAuth2 Client Credentials simplificado:
        1. Prepara dados da requisi√ß√£o
        2. Faz autentica√ß√£o com credenciais configuradas
        3. Extrai access_token da resposta
        4. Armazena token para uso posterior
        
        FLUXO OAUTH2 CLIENT CREDENTIALS:
        POST /oauth2/token
        - grant_type: client_credentials
        - Auth: Basic (client_id, client_secret)
        
        RESPOSTA ESPERADA:
        {
            "access_token": "jwt_token_aqui",
            "expires_in": 3600,
            "token_type": "Bearer"
        }
        
        Returns:
            bool: True se token obtido com sucesso, False caso contr√°rio
        """
        try:
            print("\nüîë Obtendo token de acesso...")
            
            # Obter URLs baseadas no ambiente (exp/prod)
            urls = self.config.get_urls()
            
            # Preparar dados OAuth2 Client Credentials
            dados = {"grant_type": "client_credentials"}
            
            # Fazer requisi√ß√£o de autentica√ß√£o
            resposta = requests.post(
                urls["token"],
                data=dados,
                auth=(self.config.CLIENT_ID, self.config.CLIENT_SECRET),
                timeout=self.config.REQUEST_TIMEOUT
            )
            
            # Verificar sucesso da autentica√ß√£o
            if resposta.status_code != 200:
                print(f"‚ùå Erro na autentica√ß√£o: {resposta.status_code}")
                print(f"Resposta: {resposta.text}")
                return False
            
            # Extrair token da resposta
            token_data = resposta.json()
            self.access_token = token_data["access_token"]
            print("‚úÖ Token obtido com sucesso")
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao obter token: {e}")
            return False
    
    def create_llm_prompt(self, justificativa: str) -> str:
        """
        CRIA√á√ÉO DO PROMPT ESPECIALIZADO PARA AN√ÅLISE SEM√ÇNTICA
        
        Cria o prompt padr√£o usado em todo o sistema para:
        1. Definir o papel do LLM como especialista em consignados
        2. Especificar crit√©rios exatos de aprova√ß√£o
        3. Listar exclus√µes (fora do escopo)
        4. Solicitar formato de resposta JSON estruturado
        5. Incluir a justificativa espec√≠fica do usu√°rio
        
        ESTRUTURA DO PROMPT:
        1. Defini√ß√£o de papel (especialista em empr√©stimos consignados)
        2. Crit√©rios de aprova√ß√£o (3 categorias principais)
        3. Exclus√µes (rediscuss√£o de contratos, boletos)
        4. Instru√ß√µes de formato JSON
        5. Especifica√ß√£o de campos obrigat√≥rios
        6. Justificativa do usu√°rio
        
        CRIT√âRIOS DE APROVA√á√ÉO:
        ‚Ä¢ Consigna√ß√£o sem autoriza√ß√£o pr√©via e formal
        ‚Ä¢ Consigna√ß√£o sem correspondente cr√©dito
        ‚Ä¢ Desconto de contrato j√° liquidado
        
        EXCLUS√ïES (DEVEM SER NEGADAS):
        ‚Ä¢ Rediscuss√£o de contrato assinado
        ‚Ä¢ Requisi√ß√µes de boletos
        
        Args:
            justificativa: Texto da justificativa enviada pelo usu√°rio
            
        Returns:
            str: Prompt completo formatado para o LLM
        """
        return f"""Voc√™ √© um especialista em empr√©stimos consignados.
Sua tarefa √© avaliar a justificativa enviada por um usu√°rio com base em um ou mais dos seguintes crit√©rios:
‚Ä¢ Consigna√ß√£o em folha sem autoriza√ß√£o pr√©via e formal do consignado;
‚Ä¢ Consigna√ß√£o em folha sem o correspondente cr√©dito do valor ao consignado;
‚Ä¢ Manuten√ß√£o de desconto em folha referente a contrato j√° liquidado;
N√£o faz parte do escopo e deve ser negado:
‚Ä¢ rediscuss√£o de contrato assinado (contrato indevido, taxas abusivas, etc.);
‚Ä¢ requisi√ß√µes de boletos;
Instru√ß√µes:
Verifique se a justificativa apresentada se enquadra em um ou mais dos crit√©rios acima.
Ao final, produza √∫nica sa√≠da no formato JSON abaixo, preenchendo todos os campos:
{{
  "requestId": "<UUID>",
  "timestamp": "<ISO 8601 com fuso -03:00>",
  "diagnosticoLLM": "SIM" | "N√ÉO",
  "justificativaLLM": "<texto livre at√© 144 caracteres>",
  "confidence": <valor num√©rico entre 0.0 e 1.0>,
  "status": "success" | "error",
}}
‚Ä¢ requestId: id da requisicao gerado aleatoriamente
‚Ä¢ timestamp: hora da execu√ß√£o
‚Ä¢ diagnosticoLLM: resposta sim ou n√£o se o texto do usu√°rio se encaixa nas categorias determinadas
‚Ä¢ justificativaLLM: racional para a resposta acima
‚Ä¢ confidence: confian√ßa na resposta do LLM
‚Ä¢ status: OK ou NOK
Abaixo, a justificativa enviada pelo usu√°rio:

{justificativa}"""
    
    async def call_serpro_llm(self, prompt: str, dados_entrada: dict = None):
        """
        CHAMADA PRINCIPAL PARA O SERPRO LLM
        
        Executa comunica√ß√£o ass√≠ncrona com Serpro LLM:
        1. Verifica se h√° token v√°lido (obt√©m se necess√°rio)
        2. Prepara headers e payload
        3. Faz requisi√ß√£o HTTP POST ass√≠ncrona
        4. Mede tempo de resposta
        5. Trata erros HTTP
        6. Parseia resposta usando estrat√©gias m√∫ltiplas
        7. Retorna TesteResult estruturado
        
        FORMATO DO PAYLOAD (OpenAI-compatible):
        {
            "model": "deepseek-r1-distill-qwen-14b",
            "messages": [{"role": "user", "content": "prompt"}],
            "temperature": 0.1,
            "max_tokens": 500,
            ...outras configura√ß√µes do LLM
        }
        
        MEDI√á√ÉO DE PERFORMANCE:
        - Tempo de resposta em segundos
        - Sucesso/falha da requisi√ß√£o
        - Tamanho da resposta
        
        Args:
            prompt: Prompt formatado para enviar ao LLM
            dados_entrada: Dados da entrada (id_termo, cpf, etc.) para resultado estruturado
            
        Returns:
            TesteResult: Resultado estruturado completo, ou None se erro
        """
        # Garantir que dados_entrada existe
        if dados_entrada is None:
            dados_entrada = {"justificativa": "Teste manual"}
            
        try:
            # 1. Garantir que temos token v√°lido
            if not self.access_token:
                if not self.get_access_token():
                    return None
            
            print("üß† Enviando para Serpro LLM...")
            
            # 2. Preparar URLs e headers
            urls = self.config.get_urls()
            headers = {
                "Authorization": f"Bearer {self.access_token}",
                "Content-Type": "application/json"
            }
            
            # 3. Preparar payload compat√≠vel com OpenAI API
            payload = {
                "model": self.config.MODEL_NAME,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                **self.config.LLM_CONFIG  # temperature, max_tokens, etc.
            }
            
            # 4. Iniciar medi√ß√£o de tempo
            start_time = time.time()
            
            # 5. Fazer requisi√ß√£o ass√≠ncrona
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{urls['api']}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=self.config.REQUEST_TIMEOUT)
                ) as response:
                    
                    # 6. Calcular tempo de resposta
                    response_time = time.time() - start_time
                    
                    # 7. Verificar sucesso
                    if response.status == 200:
                        result = await response.json()
                        print(f"‚úÖ Resposta recebida em {response_time:.2f}s")
                        return self.parse_llm_response(result, response_time, dados_entrada)
                    else:
                        # 8. Tratar erro HTTP
                        error_text = await response.text()
                        print(f"‚ùå Erro HTTP {response.status}: {error_text}")
                        return None
                        
        except Exception as e:
            print(f"‚ùå Erro na chamada LLM: {e}")
            return None
    
    def parse_llm_response(self, llm_response: dict, response_time: float, dados_entrada: dict) -> TesteResult:
        """
        PARSING INTELIGENTE DE RESPOSTA DO LLM COM M√öLTIPLAS ESTRAT√âGIAS
        
        O Serpro LLM pode retornar diferentes formatos de resposta:
        1. JSON puro e v√°lido
        2. JSON embutido em texto/markdown  
        3. Texto livre sem JSON v√°lido
        
        ESTRAT√âGIAS DE PARSING (em ordem de tentativa):
        1. **JSON DIRETO**: Se conte√∫do come√ßa com '{', tenta JSON.loads()
        2. **EXTRA√á√ÉO REGEX**: Busca padr√µes JSON no meio do texto
        3. **FALLBACK INTELIGENTE**: An√°lise sem√¢ntica por palavras-chave
        
        RESULTADO ESTRUTURADO:
        - Cria TesteResult com todos os dados do processamento
        - Inclui metadados de performance e configura√ß√£o
        - Preserva informa√ß√µes originais de entrada
        
        Args:
            llm_response: Resposta bruta do Serpro LLM
            response_time: Tempo da requisi√ß√£o em segundos
            dados_entrada: Dados originais da entrada (id_termo, cpf, etc.)
            
        Returns:
            TesteResult: Resultado estruturado completo, ou None se erro total
        """
        try:
            # Extrair conte√∫do principal da resposta
            content = llm_response["choices"][0]["message"]["content"]
            
            # Inicializar flags
            fallback_used = False
            
            # ESTRAT√âGIA 1: JSON DIRETO
            try:
                if content.strip().startswith('{'):
                    parsed_content = json.loads(content)
                else:
                    # ESTRAT√âGIA 2: EXTRA√á√ÉO VIA REGEX
                    # Regex para encontrar JSON complexo (incluindo objetos aninhados)
                    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                    if json_match:
                        parsed_content = json.loads(json_match.group())
                    else:
                        # ESTRAT√âGIA 3: FALLBACK INTELIGENTE
                        parsed_content = self.create_fallback_response(content)
                        fallback_used = True
                        
            except json.JSONDecodeError:
                # Se JSON inv√°lido, usar fallback
                parsed_content = self.create_fallback_response(content)
                fallback_used = True
            
            # Criar resultado estruturado
            resultado = TesteResult(
                id_termo=dados_entrada.get("id_termo", "MANUAL"),
                cpf=dados_entrada.get("cpf", ""),
                pratica_vedada=dados_entrada.get("pratica_vedada", ""),
                justificativa=dados_entrada.get("justificativa", ""),
                diagnostico_llm=parsed_content.get("diagnosticoLLM", ""),
                confidence=parsed_content.get("confidence", 0.0),
                justificativa_llm=parsed_content.get("justificativaLLM", ""),
                processing_time=response_time,
                model_used=self.config.MODEL_NAME,
                ambiente_serpro=self.config.AMBIENTE,
                fallback_used=fallback_used,
                request_id=parsed_content.get("requestId", str(uuid.uuid4()))
            )
            
            return resultado
            
        except Exception as e:
            print(f"‚ùå Erro ao parsear resposta: {e}")
            return None
    
    async def save_result_json(self, resultado: TesteResult):
        """
        SALVAMENTO DO RESULTADO EM ARQUIVO JSON
        
        Salva o resultado estruturado em arquivo JSON na pasta ./JSON
        com formato: idtermo_YYYYMMDD_HHMMSS.json
        
        FORMATO DO ARQUIVO:
        - Nome: {id_termo}_{data}_{hora}.json
        - Conte√∫do: Todos os dados do TesteResult em formato JSON
        - Encoding: UTF-8 
        - Formata√ß√£o: Indentado para legibilidade
        
        Args:
            resultado: TesteResult com dados completos do processamento
        """
        try:
            # Gerar timestamp para nome do arquivo
            now = datetime.now()
            timestamp_str = now.strftime("%Y%m%d_%H%M%S")
            
            # Criar nome do arquivo
            filename = f"{resultado.id_termo}_{timestamp_str}.json"
            filepath = self.json_folder / filename
            
            # Salvar resultado em JSON
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(asdict(resultado), f, indent=2, ensure_ascii=False)
            
            print(f"üíæ Resultado salvo em: {filepath}")
            
        except Exception as e:
            print(f"‚ùå Erro ao salvar JSON: {e}")
    
    def create_fallback_response(self, content: str):
        """
        SISTEMA DE FALLBACK INTELIGENTE PARA AN√ÅLISE SEM√ÇNTICA
        
        Quando o LLM n√£o retorna JSON v√°lido, este m√©todo faz:
        1. An√°lise sem√¢ntica do texto por palavras-chave
        2. Contagem de indicadores de aprova√ß√£o vs rejei√ß√£o
        3. C√°lculo de diagn√≥stico baseado na an√°lise
        4. Estimativa de confian√ßa baseada na for√ßa dos indicadores
        5. Gera√ß√£o de resposta no formato padr√£o
        
        ALGORITMO DE AN√ÅLISE:
        - **Palavras de Aprova√ß√£o**: sim, v√°lido, procedente, autoriza√ß√£o, etc.
        - **Palavras de Rejei√ß√£o**: n√£o, inv√°lido, taxa, boleto, etc.
        - **Diagn√≥stico**: Categoria com mais ocorr√™ncias
        - **Confian√ßa**: Baseada na quantidade e for√ßa dos indicadores
        
        C√ÅLCULO DE CONFIAN√áA:
        - Base: 0.5 (neutro)
        - +0.1 por palavra indicativa (m√°ximo 0.9)
        - Limitado a 0.9 para manter margem de incerteza
        
        Args:
            content: Texto livre da resposta do LLM
            
        Returns:
            dict: Resposta no formato padr√£o com diagn√≥stico inferido
        """
        content_lower = content.lower()
        
        # Listas de palavras-chave para an√°lise sem√¢ntica
        approve_words = ["sim", "aprovado", "v√°lido", "procedente", "autoriza√ß√£o", "liquidado", "cr√©dito"]
        reject_words = ["n√£o", "rejeitado", "inv√°lido", "taxa", "boleto", "renegociar"]
        
        # Contar ocorr√™ncias de cada categoria
        approve_count = sum(1 for word in approve_words if word in content_lower)
        reject_count = sum(1 for word in reject_words if word in content_lower)
        
        # Determinar diagn√≥stico baseado na an√°lise
        if approve_count > reject_count:
            diagnostico = "SIM"
            # Confian√ßa baseada na for√ßa dos indicadores (m√°ximo 0.9)
            confidence = min(0.9, 0.5 + (approve_count * 0.1))
        else:
            diagnostico = "N√ÉO"
            confidence = min(0.9, 0.5 + (reject_count * 0.1))
            
        # Retornar resposta no formato padr√£o
        return {
            "requestId": str(uuid.uuid4()),
            "timestamp": datetime.now().strftime("%Y-%m-%dT%H:%M:%S-03:00"),
            "diagnosticoLLM": diagnostico,
            "justificativaLLM": content[:144],  # Limitar a 144 caracteres
            "confidence": confidence,
            "status": "success"
        }

# ========== FUN√á√ÉO PRINCIPAL INTERATIVA ==========

async def main():
    """
    LOOP PRINCIPAL INTERATIVO DO TESTE MANUAL
    
    Implementa interface de linha de comando para:
    1. Inicializa√ß√£o e exibi√ß√£o de configura√ß√µes
    2. Loop interativo para entrada de justificativas/linhas completas
    3. Processamento e exibi√ß√£o de resultados
    4. Controle de continua√ß√£o/sa√≠da
    5. Salvamento autom√°tico quando linha completa
    
    FLUXO DE EXECU√á√ÉO:
    1. **Inicializa√ß√£o**:
       - Cria inst√¢ncia de TesteLLMManual
       - Exibe configura√ß√µes atuais
       - Configura certificados automaticamente
    
    2. **Loop Interativo**:
       - Solicita entrada do usu√°rio
       - Detecta formato (justificativa simples ou linha completa)
       - Para linha completa: parseia dados e salva JSON
       - Para justificativa simples: processa normalmente
       - Cria prompt especializado
       - Chama Serpro LLM
       - Parseia e exibe resposta
       - Pergunta se quer continuar
    
    3. **Exibi√ß√£o de Resultados**:
       - JSON formatado completo
       - Resumo executivo
       - M√©tricas de performance
       - Alertas sobre fallback
    
    COMANDOS DE SA√çDA:
    - 'sair', 'exit', 'quit', '' (vazio)
    
    COMANDOS DE CONTINUA√á√ÉO:
    - Enter: continuar
    - 'n', 'no', 'nao', 'n√£o': parar
    """
    print("üß™ TESTE MANUAL SERPRO LLM")
    print("=" * 50)
    
    # 1. Inicializar sistema de teste
    teste = TesteLLMManual()
    
    # 2. Exibir configura√ß√µes atuais
    print(f"ü§ñ Modelo: {teste.config.MODEL_NAME}")
    print(f"üåê Ambiente: {teste.config.AMBIENTE}")
    print(f"‚è±Ô∏è Timeout: {teste.config.REQUEST_TIMEOUT}s")
    print(f"üìÅ Pasta JSON: {teste.json_folder}")
    
    # 3. Loop principal interativo
    while True:
        print("\n" + "=" * 50)
        print("üìù DIGITE SUA ENTRADA")
        print("=" * 50)
        print("Formatos aceitos:")
        print("1. Justificativa simples: 'Estou sendo descontado sem autoriza√ß√£o'")
        print("2. Linha completa: 'TERMO123#12345678901#12#Desconto sem autoriza√ß√£o'")
        print()
        
        # 4. Solicitar entrada do usu√°rio
        print("Digite sua entrada (ou 'sair' para terminar):")
        entrada = input("> ").strip()
        
        # 5. Verificar comandos de sa√≠da
        if entrada.lower() in ['sair', 'exit', 'quit', '']:
            print("üëã Encerrando teste...")
            break
        
        # 6. Detectar formato da entrada
        formato = teste.detect_input_format(entrada)
        print(f"\nüîç Formato detectado: {formato}")
        
        # 7. Processar entrada baseado no formato
        if formato == "linha_completa":
            try:
                # Parsear linha completa
                dados_entrada = teste.parse_linha_completa(entrada)
                print(f"üìã Dados parseados:")
                print(f"   ID Termo: {dados_entrada['id_termo']}")
                print(f"   CPF: {dados_entrada['cpf']}")
                print(f"   Pr√°tica Vedada: {dados_entrada['pratica_vedada']}")
                print(f"   Justificativa: {dados_entrada['justificativa'][:100]}...")
                
                # Usar justificativa para o prompt
                justificativa_para_prompt = dados_entrada['justificativa']
                
            except ValueError as e:
                print(f"‚ùå Erro no formato da linha: {e}")
                continue
        else:
            # Entrada simples - toda a entrada √© a justificativa
            dados_entrada = {"justificativa": entrada}
            justificativa_para_prompt = entrada
            print(f"üìã Justificativa recebida: {entrada[:100]}...")
        
        # 8. Criar prompt especializado
        prompt = teste.create_llm_prompt(justificativa_para_prompt)
        
        # 9. Chamar Serpro LLM
        resultado = await teste.call_serpro_llm(prompt, dados_entrada)
        
        # 10. Processar e exibir resultados
        if resultado:
            print("\n" + "=" * 50)
            print("üìä RESPOSTA DO SERPRO LLM")
            print("=" * 50)
            
            # 11. Exibir JSON formatado completo
            print(json.dumps(asdict(resultado), indent=2, ensure_ascii=False))
            
            # 12. Salvar JSON se linha completa
            if formato == "linha_completa":
                await teste.save_result_json(resultado)
            
            # 13. Exibir resumo executivo
            print("\nüìà RESUMO:")
            print(f"   üéØ Diagn√≥stico: {resultado.diagnostico_llm}")
            print(f"   üìä Confian√ßa: {resultado.confidence:.2f}")
            print(f"   ‚è±Ô∏è Tempo: {resultado.processing_time:.2f}s")
            print(f"   üß† Justificativa: {resultado.justificativa_llm[:144]}.")
            
            # 14. Alertar sobre uso de fallback
            if resultado.fallback_used:
                print("   ‚ö†Ô∏è Fallback usado (LLM n√£o retornou JSON v√°lido)")
                
            # 15. Mostrar metadados adicionais
            print(f"   üÜî Request ID: {resultado.request_id}")
            print(f"   ü§ñ Modelo: {resultado.model_used}")
            print(f"   üåê Ambiente: {resultado.ambiente_serpro}")
            
        else:
            print("‚ùå Falha na comunica√ß√£o com Serpro LLM")
        
        # 16. Perguntar sobre continua√ß√£o
        print("\nüîÑ Deseja fazer outro teste? (Enter = sim, 'n' = n√£o)")
        continuar = input("> ").strip().lower()
        if continuar in ['n', 'no', 'nao', 'n√£o']:
            break

# ========== FUN√á√ÉO WRAPPER S√çNCRONA ==========

def run_teste():
    """
    WRAPPER S√çNCRONO PARA EXECU√á√ÉO DO TESTE
    
    Executa a fun√ß√£o ass√≠ncrona main() em um event loop:
    - Trata interrup√ß√£o por Ctrl+C gracefully
    - Captura e exibe erros fatais
    - Garante cleanup adequado
    
    TRATAMENTO DE EXCE√á√ïES:
    - KeyboardInterrupt: Usu√°rio pressionou Ctrl+C
    - Exception: Outros erros inesperados
    """
    try:
        # Executar fun√ß√£o principal ass√≠ncrona
        asyncio.run(main())
    except KeyboardInterrupt:
        # Interrup√ß√£o pelo usu√°rio (Ctrl+C)
        print("\nüõë Teste interrompido pelo usu√°rio")
    except Exception as e:
        # Outros erros inesperados
        print(f"\nüí• Erro fatal: {e}")

# ========== PONTO DE ENTRADA ==========

if __name__ == "__main__":
    """
    EXECU√á√ÉO PRINCIPAL DO SCRIPT
    
    Ponto de entrada quando script √© executado diretamente:
    - Chama fun√ß√£o wrapper s√≠ncrona
    - Permite execu√ß√£o via: python H_teste_manual_llm.py
    """
    run_teste()